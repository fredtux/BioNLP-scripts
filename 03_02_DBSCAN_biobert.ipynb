{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537f15cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6325f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dir = Path('dataset') / 'biobert_split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4742d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(338, 85, 338, 85, 338, 85)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.load(split_dir / 'X_train.npy')\n",
    "X_train_sent = np.load(split_dir / 'X_train_sent.npy')\n",
    "X_test = np.load(split_dir / 'X_test.npy')\n",
    "X_test_sent = np.load(split_dir / 'X_test_sent.npy')\n",
    "y_train = np.load(split_dir / 'y_train.npy')\n",
    "y_test = np.load(split_dir / 'y_test.npy')\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test), len(X_train_sent), len(X_test_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4635582c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('em ##etophobia flying fly motion over the years husband 7 – 8 hour books 1 hour na husband regressed every day motion fly feel trapped <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n",
       " 'intense emetophobia freaking out close boba tea stomach ache panic tired few hours later went to sleep woke up house ill fine eat stomach rumble almost 1 in the afternoon 30 minutes sick z ##ofran pep sick <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = [' '.join(row) for row in X_train]\n",
    "X_test = [' '.join(row) for row in X_test]\n",
    "\n",
    "X_train[0], X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8824596",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_model = Word2Vec(sentences=X_train, vector_size=64, window=10, min_count=1, sg=1, workers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcca8955",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66e7de1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 338/338 [00:00<00:00, 7681.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 -0.7819 64\n",
      "65 -0.9469 64\n",
      "65 -0.9586 64\n",
      "65 -0.7489 64\n",
      "65 0.4194 64\n",
      "65 -0.9179 64\n",
      "65 0.296 64\n",
      "65 -0.9843 64\n",
      "65 0.7722 64\n",
      "65 -0.823 64\n",
      "65 0.3687 64\n",
      "65 -0.9581 64\n",
      "65 -0.98 64\n",
      "65 -0.8762 64\n",
      "65 -0.9589 64\n",
      "65 -0.9511 64\n",
      "65 -0.981 64\n",
      "65 -0.8628 64\n",
      "65 -0.9852 64\n",
      "65 -0.6124 64\n",
      "65 -0.8976 64\n",
      "65 -0.9678 64\n",
      "65 -0.9442 64\n",
      "65 -0.962 64\n",
      "65 -0.5858 64\n",
      "65 -0.9774 64\n",
      "65 -0.7619 64\n",
      "65 0.8733 64\n",
      "65 -0.9597 64\n",
      "65 -0.4401 64\n",
      "65 -0.8182 64\n",
      "65 0.2444 64\n",
      "65 -0.7684 64\n",
      "65 -0.984 64\n",
      "65 -0.8664 64\n",
      "65 -0.8534 64\n",
      "65 0.2991 64\n",
      "65 -0.6948 64\n",
      "65 0.6936 64\n",
      "65 -0.5472 64\n",
      "65 -0.9194 64\n",
      "65 -0.8878 64\n",
      "65 0.4056 64\n",
      "65 -0.8053 64\n",
      "65 0.7305 64\n",
      "65 -0.9588 64\n",
      "65 -0.6329 64\n",
      "65 -0.8885 64\n",
      "65 -0.9166 64\n",
      "65 -0.9818 64\n",
      "65 -0.963 64\n",
      "65 -0.5341 64\n",
      "65 -0.9152 64\n",
      "65 -0.8016 64\n",
      "65 -0.9933 64\n",
      "65 0.9334 64\n",
      "65 -0.898 64\n",
      "65 -0.8844 64\n",
      "65 -0.9961 64\n",
      "65 -0.8526 64\n",
      "65 0.5927 64\n",
      "65 0.7992 64\n",
      "65 -0.296 64\n",
      "65 -0.7269 64\n",
      "65 0.1027 64\n",
      "65 -0.9887 64\n",
      "65 -0.9769 64\n",
      "65 -0.8335 64\n",
      "65 -0.9029 64\n",
      "65 -0.3919 64\n",
      "65 -0.6261 64\n",
      "65 -0.1178 64\n",
      "65 -0.9528 64\n",
      "65 -0.6705 64\n",
      "65 0.0875 64\n",
      "65 0.4161 64\n",
      "65 -0.9735 64\n",
      "65 -0.975 64\n",
      "65 -0.9783 64\n",
      "65 -0.9178 64\n",
      "65 -0.5272 64\n",
      "65 -0.9762 64\n",
      "65 -0.9838 64\n",
      "65 -0.8463 64\n",
      "65 0.8609 64\n",
      "65 -0.9497 64\n",
      "65 -0.954 64\n",
      "65 -0.821 64\n",
      "65 -0.5985 64\n",
      "65 -0.8858 64\n",
      "65 -0.7184 64\n",
      "65 -0.513 64\n",
      "65 -0.905 64\n",
      "65 0.6486 64\n",
      "65 -0.9714 64\n",
      "65 -0.8358 64\n",
      "65 -0.9398 64\n",
      "65 -0.9514 64\n",
      "65 -0.9341 64\n",
      "65 -0.9893 64\n",
      "65 -0.9921 64\n",
      "65 0.0377 64\n",
      "65 -0.8078 64\n",
      "65 -0.9885 64\n",
      "65 -0.4588 64\n",
      "65 -0.7852 64\n",
      "65 -0.3699 64\n",
      "65 -0.9485 64\n",
      "65 0.9001 64\n",
      "65 0.7482 64\n",
      "65 0.774 64\n",
      "65 0.9186 64\n",
      "65 -0.7873 64\n",
      "65 -0.5513 64\n",
      "65 -0.9614 64\n",
      "65 -0.5732 64\n",
      "65 0.9721 64\n",
      "65 -0.956 64\n",
      "65 -0.8732 64\n",
      "65 0.2683 64\n",
      "65 -0.9739 64\n",
      "65 -0.961 64\n",
      "65 0.8818 64\n",
      "65 0.2196 64\n",
      "65 -0.7868 64\n",
      "65 -0.4295 64\n",
      "65 -0.8466 64\n",
      "65 -0.255 64\n",
      "65 -0.5574 64\n",
      "65 -0.9664 64\n",
      "65 -0.9274 64\n",
      "65 -0.8834 64\n",
      "65 -0.8691 64\n",
      "65 0.4347 64\n",
      "65 0.6307 64\n",
      "65 -0.228 64\n",
      "65 -0.976 64\n",
      "65 -0.8839 64\n",
      "65 -0.995 64\n",
      "65 0.3415 64\n",
      "65 0.6486 64\n",
      "65 -0.3962 64\n",
      "65 -0.9539 64\n",
      "65 -0.9771 64\n",
      "65 -0.336 64\n",
      "65 -0.9682 64\n",
      "65 -0.8934 64\n",
      "65 -0.9923 64\n",
      "65 -0.8519 64\n",
      "65 -0.9958 64\n",
      "65 -0.2614 64\n",
      "65 -0.8939 64\n",
      "65 0.9062 64\n",
      "65 -0.9851 64\n",
      "65 -0.8957 64\n",
      "65 -0.9039 64\n",
      "65 -0.928 64\n",
      "65 -0.0405 64\n",
      "65 -0.9572 64\n",
      "65 0.9639 64\n",
      "65 -0.9195 64\n",
      "65 -0.9966 64\n",
      "65 -0.9166 64\n",
      "65 0.3801 64\n",
      "65 -0.5049 64\n",
      "65 -0.9865 64\n",
      "65 0.9492 64\n",
      "65 -0.9302 64\n",
      "65 -0.9357 64\n",
      "65 -0.9728 64\n",
      "65 0.1403 64\n",
      "65 -0.8874 64\n",
      "65 -0.9084 64\n",
      "65 -0.0516 64\n",
      "65 -0.9899 64\n",
      "65 -0.9667 64\n",
      "65 0.4957 64\n",
      "65 -0.8156 64\n",
      "65 -0.833 64\n",
      "65 -0.5549 64\n",
      "65 -0.9699 64\n",
      "65 -0.6641 64\n",
      "65 -0.8641 64\n",
      "65 -0.8615 64\n",
      "65 -0.9935 64\n",
      "65 0.8811 64\n",
      "65 -0.128 64\n",
      "65 -0.9304 64\n",
      "65 0.8902 64\n",
      "65 -0.9751 64\n",
      "65 -0.9348 64\n",
      "65 -0.9076 64\n",
      "65 -0.8603 64\n",
      "65 0.3509 64\n",
      "65 -0.8558 64\n",
      "65 -0.6745 64\n",
      "65 0.1134 64\n",
      "65 0.9801 64\n",
      "65 -0.9773 64\n",
      "65 -0.9866 64\n",
      "65 -0.9081 64\n",
      "65 -0.8152 64\n",
      "65 0.8273 64\n",
      "65 -0.8261 64\n",
      "65 -0.9688 64\n",
      "65 -0.8156 64\n",
      "65 -0.9957 64\n",
      "65 -0.6332 64\n",
      "65 0.8883 64\n",
      "65 -0.7712 64\n",
      "65 0.5067 64\n",
      "65 -0.973 64\n",
      "65 -0.9915 64\n",
      "65 -0.8395 64\n",
      "65 -0.8049 64\n",
      "65 -0.9855 64\n",
      "65 -0.9261 64\n",
      "65 -0.9199 64\n",
      "65 -0.8587 64\n",
      "65 0.7197 64\n",
      "65 -0.7139 64\n",
      "65 -0.7171 64\n",
      "65 -0.9332 64\n",
      "65 0.6749 64\n",
      "65 -0.6174 64\n",
      "65 -0.6072 64\n",
      "65 -0.6772 64\n",
      "65 0.871 64\n",
      "65 -0.9015 64\n",
      "65 -0.9961 64\n",
      "65 -0.6701 64\n",
      "65 -0.9654 64\n",
      "65 -0.3893 64\n",
      "65 -0.4324 64\n",
      "65 -0.7143 64\n",
      "65 -0.9484 64\n",
      "65 -0.3182 64\n",
      "65 0.9086 64\n",
      "65 -0.7367 64\n",
      "65 0.5023 64\n",
      "65 -0.9679 64\n",
      "65 -0.9219 64\n",
      "65 -0.5218 64\n",
      "65 -0.7304 64\n",
      "65 -0.6511 64\n",
      "65 -0.8731 64\n",
      "65 0.981 64\n",
      "65 0.7191 64\n",
      "65 -0.994 64\n",
      "65 0.9492 64\n",
      "65 -0.8798 64\n",
      "65 -0.9941 64\n",
      "65 0.5027 64\n",
      "65 -0.9832 64\n",
      "65 -0.9854 64\n",
      "65 0.9065 64\n",
      "65 -0.9549 64\n",
      "65 0.5726 64\n",
      "65 -0.9433 64\n",
      "65 -0.91 64\n",
      "65 0.0163 64\n",
      "65 -0.3597 64\n",
      "65 -0.9033 64\n",
      "65 -0.889 64\n",
      "65 -0.8407 64\n",
      "65 -0.9872 64\n",
      "65 -0.802 64\n",
      "65 0.318 64\n",
      "65 -0.8033 64\n",
      "65 -0.296 64\n",
      "65 -0.9869 64\n",
      "65 -0.0772 64\n",
      "65 0.7726 64\n",
      "65 -0.9601 64\n",
      "65 -0.9508 64\n",
      "65 -0.7452 64\n",
      "65 -0.9822 64\n",
      "65 -0.8608 64\n",
      "65 -0.991 64\n",
      "65 -0.9986 64\n",
      "65 0.7723 64\n",
      "65 -0.8402 64\n",
      "65 -0.9277 64\n",
      "65 -0.9885 64\n",
      "65 0.4879 64\n",
      "65 -0.7941 64\n",
      "65 -0.9897 64\n",
      "65 -0.8945 64\n",
      "65 0.1007 64\n",
      "65 -0.6588 64\n",
      "65 0.9095 64\n",
      "65 -0.9366 64\n",
      "65 0.9908 64\n",
      "65 0.433 64\n",
      "65 -0.7096 64\n",
      "65 -0.3347 64\n",
      "65 -0.9818 64\n",
      "65 -0.8099 64\n",
      "65 -0.6204 64\n",
      "65 0.4423 64\n",
      "65 -0.4075 64\n",
      "65 -0.5651 64\n",
      "65 -0.0152 64\n",
      "65 0.9287 64\n",
      "65 -0.8095 64\n",
      "65 -0.9398 64\n",
      "65 -0.8107 64\n",
      "65 -0.9444 64\n",
      "65 -0.6689 64\n",
      "65 0.9716 64\n",
      "65 -0.1408 64\n",
      "65 0.4404 64\n",
      "65 -0.9736 64\n",
      "65 -0.9581 64\n",
      "65 -0.9727 64\n",
      "65 -0.9399 64\n",
      "65 -0.8317 64\n",
      "65 -0.9466 64\n",
      "65 -0.9289 64\n",
      "65 -0.9804 64\n",
      "65 -0.923 64\n",
      "65 -0.9216 64\n",
      "65 -0.875 64\n",
      "65 0.9411 64\n",
      "65 0.96 64\n",
      "65 -0.7495 64\n",
      "65 0.8534 64\n",
      "65 -0.8271 64\n",
      "65 -0.4693 64\n",
      "65 -0.8779 64\n",
      "65 0.4595 64\n",
      "65 -0.9767 64\n",
      "65 -0.8943 64\n",
      "65 -0.9906 64\n",
      "65 -0.8401 64\n",
      "65 -0.9245 64\n",
      "65 0.9294 64\n",
      "65 -0.9062 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [00:00<00:00, 9923.89it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_final = []\n",
    "X_test_final = []\n",
    "\n",
    "for i in tqdm(range(len(X_train))):\n",
    "    text = X_train[i]\n",
    "    sentiment = X_train_sent[i]\n",
    "\n",
    "    tokens = text.split()\n",
    "    vectors = [skipgram_model.wv[token] for token in tokens if token in skipgram_model.wv]\n",
    "\n",
    "    if len(vectors) > train_vec_size:\n",
    "        vectors = vectors[:train_vec_size]\n",
    "    elif len(vectors) < train_vec_size:\n",
    "        vectors += [np.zeros(64)] * (train_vec_size - len(vectors))\n",
    "\n",
    "    combined_vector = np.concatenate((np.mean(vectors, axis=0), np.array([sentiment])))\n",
    "    print(len(combined_vector), sentiment, len(vectors))\n",
    "\n",
    "    X_train_final.append(combined_vector)\n",
    "\n",
    "for i in tqdm(range(len(X_test))):\n",
    "    text = X_test[i]\n",
    "    sentiment = X_test_sent[i]\n",
    "\n",
    "    tokens = text.split()\n",
    "    vectors = [skipgram_model.wv[token] for token in tokens if token in skipgram_model.wv]\n",
    "\n",
    "    if len(vectors) > train_vec_size:\n",
    "        vectors = vectors[:train_vec_size]\n",
    "    elif len(vectors) < train_vec_size:\n",
    "        vectors += [np.zeros(64)] * (train_vec_size - len(vectors))\n",
    "\n",
    "    combined_vector = np.concatenate((np.mean(vectors, axis=0), np.array([sentiment])))\n",
    "\n",
    "    X_test_final.append(combined_vector)\n",
    "\n",
    "X_train_final = np.array(X_train_final)\n",
    "X_train_final = X_train_final.astype(np.float64)\n",
    "X_test_final = np.array(X_test_final)\n",
    "X_test_final = X_test_final.astype(np.float64)\n",
    "\n",
    "# len(X_train_final), len(X_test_final), len(y_train), len(y_test), X_train_final[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caf0445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = len(['Question', 'Needing support - Panic attack', 'Rant', 'Potentially Triggering', 'Does Anyone Else...?', 'Needing support: Just not feeling good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dff8b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration 0, inertia 4.0297757189224805.\n",
      "Iteration 1, inertia 2.228868438212303.\n",
      "Iteration 2, inertia 2.108934177114295.\n",
      "Iteration 3, inertia 2.089351329429714.\n",
      "Iteration 4, inertia 2.0808854640302297.\n",
      "Iteration 5, inertia 2.0739314279404164.\n",
      "Iteration 6, inertia 2.0630802719314896.\n",
      "Iteration 7, inertia 2.0574672758099495.\n",
      "Iteration 8, inertia 2.05199262958622.\n",
      "Iteration 9, inertia 2.0381306896138787.\n",
      "Iteration 10, inertia 2.036892043031641.\n",
      "Iteration 11, inertia 2.0361440763871212.\n",
      "Iteration 12, inertia 2.0360331203334043.\n",
      "Converged at iteration 12: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.3039618526694103.\n",
      "Iteration 1, inertia 1.9653779013536983.\n",
      "Iteration 2, inertia 1.8630965923517853.\n",
      "Iteration 3, inertia 1.8394791970596622.\n",
      "Iteration 4, inertia 1.837362402185891.\n",
      "Iteration 5, inertia 1.831513782770773.\n",
      "Iteration 6, inertia 1.8296351719514456.\n",
      "Iteration 7, inertia 1.8293381008498193.\n",
      "Iteration 8, inertia 1.8289248638384588.\n",
      "Iteration 9, inertia 1.8254978613166626.\n",
      "Iteration 10, inertia 1.82108750868579.\n",
      "Iteration 11, inertia 1.8183222255665639.\n",
      "Iteration 12, inertia 1.815720728642062.\n",
      "Iteration 13, inertia 1.8142127989762968.\n",
      "Iteration 14, inertia 1.814016206075342.\n",
      "Converged at iteration 14: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.522260266393514.\n",
      "Iteration 1, inertia 2.049700402075348.\n",
      "Iteration 2, inertia 2.013433258281263.\n",
      "Iteration 3, inertia 1.9583292614774566.\n",
      "Iteration 4, inertia 1.9018769887390001.\n",
      "Iteration 5, inertia 1.861032958651735.\n",
      "Iteration 6, inertia 1.8383344502355954.\n",
      "Iteration 7, inertia 1.8381929745721226.\n",
      "Converged at iteration 7: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 3.7488633963672906.\n",
      "Iteration 1, inertia 2.143230846137667.\n",
      "Iteration 2, inertia 1.930071395527554.\n",
      "Iteration 3, inertia 1.8790172356967105.\n",
      "Iteration 4, inertia 1.8438552070878025.\n",
      "Iteration 5, inertia 1.8360294285538565.\n",
      "Iteration 6, inertia 1.8329614770501141.\n",
      "Iteration 7, inertia 1.8271128576349964.\n",
      "Iteration 8, inertia 1.825234246815669.\n",
      "Iteration 9, inertia 1.8249371757140427.\n",
      "Iteration 10, inertia 1.8245239387026821.\n",
      "Iteration 11, inertia 1.8210969361808858.\n",
      "Iteration 12, inertia 1.8166865835500134.\n",
      "Iteration 13, inertia 1.8139213004307875.\n",
      "Iteration 14, inertia 1.8113198035062852.\n",
      "Iteration 15, inertia 1.8098118738405202.\n",
      "Iteration 16, inertia 1.8096152809395651.\n",
      "Converged at iteration 16: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.892035124583466.\n",
      "Iteration 1, inertia 2.028895022926032.\n",
      "Iteration 2, inertia 1.889144407435675.\n",
      "Iteration 3, inertia 1.8235489186479372.\n",
      "Iteration 4, inertia 1.8090570275789646.\n",
      "Iteration 5, inertia 1.8044695057984423.\n",
      "Converged at iteration 5: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.422165816873334.\n",
      "Iteration 1, inertia 1.8614015901327563.\n",
      "Iteration 2, inertia 1.8289419534618772.\n",
      "Iteration 3, inertia 1.8189325042360918.\n",
      "Iteration 4, inertia 1.8128676311826077.\n",
      "Iteration 5, inertia 1.8084559568129142.\n",
      "Iteration 6, inertia 1.8081350937067096.\n",
      "Iteration 7, inertia 1.807266758213386.\n",
      "Iteration 8, inertia 1.8044196313143948.\n",
      "Iteration 9, inertia 1.8031827457474057.\n",
      "Converged at iteration 9: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.2844904656548093.\n",
      "Iteration 1, inertia 1.9332206408182433.\n",
      "Iteration 2, inertia 1.8547556098558475.\n",
      "Iteration 3, inertia 1.8318553096517396.\n",
      "Iteration 4, inertia 1.8215426497916545.\n",
      "Iteration 5, inertia 1.8123139089799611.\n",
      "Iteration 6, inertia 1.8086754940906842.\n",
      "Iteration 7, inertia 1.806374910329516.\n",
      "Iteration 8, inertia 1.8044695057984426.\n",
      "Converged at iteration 8: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.6220225488886646.\n",
      "Iteration 1, inertia 2.2901706950630962.\n",
      "Iteration 2, inertia 2.278516885983903.\n",
      "Iteration 3, inertia 2.2712084610576873.\n",
      "Iteration 4, inertia 2.2618669037527757.\n",
      "Iteration 5, inertia 2.257520346113036.\n",
      "Iteration 6, inertia 2.257057131109746.\n",
      "Converged at iteration 6: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.363382792366442.\n",
      "Iteration 1, inertia 1.9262640285940738.\n",
      "Iteration 2, inertia 1.8713157830281244.\n",
      "Iteration 3, inertia 1.841765723859631.\n",
      "Iteration 4, inertia 1.8383344502355956.\n",
      "Iteration 5, inertia 1.8381929745721224.\n",
      "Converged at iteration 5: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 2.4702276412549584.\n",
      "Iteration 1, inertia 1.828965114213116.\n",
      "Iteration 2, inertia 1.8193850852507913.\n",
      "Iteration 3, inertia 1.8167744913782466.\n",
      "Iteration 4, inertia 1.814212798976297.\n",
      "Iteration 5, inertia 1.8140162060753418.\n",
      "Converged at iteration 5: strict convergence.\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10, max_iter=300, verbose=1)\n",
    "\n",
    "y_train_pred = kmeans.fit_predict(X_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddc89f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score for training set: 0.634794811808537\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_train = silhouette_score(X_train_final, y_train_pred)\n",
    "print(f'Silhouette score for training set: {silhouette_train}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
